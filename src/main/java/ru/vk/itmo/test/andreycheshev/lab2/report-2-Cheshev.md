## Параметры системы:
* Treshhold bytes = 1000000
* Количество core потоков = 6
* Максимальное количество потоков = 60
* Размер очереди = 100
* Max heap size = 128m

## Проведем сравнение реализаций на стабильной нагрузке

### Сравним работу при -c 1 -t 1 -d 30
* Get (рандомный выбор id)
	*  -R 2550 ![](c1-t1-2550-get-cmp.png) Видно, что новая реализация значительно уступает (rps = 2550 является точкой разладки). 

* Put
	* -R 2500 ![](c1-t1-2500-put-cmp.png) Исходя из графика, видно, что результат практически идентичен.

### Сравним работу при -c 64 -t 1 -d 30 -R 2500
* Get (рандомный выбор id)
	* -R 2550 ![](c64-t1-2550-get-cmp.png)

* Put
	* -R 2500 ![](c64-t1-2500-put-cmp.png)

### Постепенно увеличим rps при -c 64 -t 1 -d 30
* Get (рандомный выбор id)
	* -R 10000 ![](c64-t1-10000-get-cmp.png)
	* -R 20000 ![](c64-t1-20000-get-cmp.png)
	* -R 25000 ![](c64-t1-25000-get-cmp.png)
	* -R 30000 ![](c64-t1-30000-get-cmp.png)
В новой реализации точка разладки при таких параметрах ниже чем в старой реализации.

* Put
	* -R 10000 ![](c64-t1-10000-put-cmp.png)
	* -R 15000 ![](c64-t1-15000-put-cmp.png)

Видим, что старая реализация практически не уступает новой, а зачастую даже превосходит.

Стал очевиден факт, что при увеличении количества соединений, появляется возможность увеличивать суммарное количество запросов в секунду.

Чем больше соединений, тем больше клиентских сокетов и сессий, к которым они привязаны. А значит, распределив запросы по сокетам мы получаем, что на каждый сокет приходит меньше запросов в секунду. В старой реализации вся пропускная способность упиралась в последовательную обработку запросов SelectorThread'ами. Сейчас же нагрузка распределятся на пул воркеров. Однако текущие параметры системы дают неудовлетворительный результат. Постараемся еще увеличить нагрузку, изменяя значение параметров.

## Увеличим на порядок количество соединений и количество запросов в секунду.

* Get (рандомный выбор id) 
	* -c 150 -t 1 -d 60 -R 33000 
	При тестировании было обнаружено, что установленный размер очереди в 100 элементов способствует ее переполнению.
	* Протестируем изменение размера очереди:
	![](c150-t1-33000_get_queue_size_cmp.png)
	Видно, что в данной конфигурации оптимальный размер очереди примерно составляет 300 элементов. Чем меньше размер очереди, тем меньшее количество задач нужно для того, чтобы ее заполнить, что будет являться сигналом к увеличению числа потоков для пула воркеров. В свою очередь, чем больше очередь, тем медленнее будут создаваться новые воркеры. Полученный результат можно объяснить тем, что очередь с максимальным размером 300 определяет оптимальное состояние системы, при котором существует баланс между нагрузкой и количеством воркеров, которое также тоже должно быть оптимальным, поскольку увеличение их количества несет за собой увеличение накладных расходов на их обслуживание.

## Исследуем зависимость производительности от количества соединений при одном и том же количестве rps.

...